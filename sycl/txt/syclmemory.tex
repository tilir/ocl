\documentclass[a4paper,12pt,oneside]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1,T2A]{fontenc}
\usepackage[english,russian]{babel}
\usepackage[usenames]{xcolor}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{cmap}
\usepackage{indentfirst}
\usepackage{imakeidx}
\usepackage[unicode]{hyperref}

\hypersetup{
%bookmarks=true,            % show bookmarks bar?
%unicode=false,             % non-Latin characters in Acrobat’s bookmarks
pdfproducer={Producer},    % producer of the document
pdfkeywords={keywords},    % list of keywords
pdfnewwindow=true,         % links in new window
colorlinks=true,           % false: boxed links; true: colored links
linkcolor=black,           % color of internal links
citecolor=black,           % color of links to bibliography
    filecolor=black,           % color of file links
    urlcolor=blue             % color of external links
}

%\begin{lstlisting}[caption={},label={lst:}]
%\end{lstlisting}

\definecolor{olivegreen}{cmyk}{0.64,0,0.95,0.40}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\setlength{\parskip}{6pt}

\lstset{
language=C++,                           % Code langugage
basicstyle=\ttfamily,                   % Code font, Examples: \footnotesize, \ttfamily
keywordstyle=\color{olivegreen},        % Keywords font ('*' = uppercase)
commentstyle=\color{gray},              % Comments font
stringstyle=\color{mauve},
numbers=left,                           % Line nums position
numberstyle=\tiny,
numbersep=10pt,
stepnumber=1,                           % Step between two line-numbers
frame=none,                             % A frame around the code
tabsize=2,                              % Default tab size
captionpos=b,                           % Caption-position = bottom
breaklines=true,                        % Automatic line breaking?
breakatwhitespace=false,                % Automatic breaks only at whitespace?
showspaces=false,                       % Dont make spaces visible
showstringspaces=false,
showtabs=false,                         % Dont make tabls visible
columns=flexible,                       % Column format
title=\lstname,
caption={},
extendedchars=\true,
inputencoding=utf8,
}

\makeindex[intoc]

\title{Память как концепция в гетерогенных системах}
\author{
  Владимиров Константин Игоревич\\
  \texttt{konstantin.vladimirov@gmail.com}
}
\date{\today}

\begin{document}

\begin{titlepage}
\begin{center}

\large ~ \\[4.5cm]

\huge Память как концепция\\[0.6cm]
\large В гетерогенных системах\\[3.7cm]

\begin{minipage}{0.5\textwidth}
\begin{flushleft}
\emph{Автор:} Владимиров К. И.\\
\end{flushleft}
\end{minipage}
\vfill
Email автора: \texttt{konstantin.vladimirov@gmail.com}\\
{\large \today}
{\large \LaTeX}

\end{center}
\thispagestyle{empty}
\end{titlepage}

\tableofcontents

\pagebreak
\section{Введение}\label{sec:Intro}

Гетерогенными системами называются системы, имеющие две выделенных роли: host и device. Host это устройство, которое иницирует вычисления и обрабатывает их результаты, device это устройство, на котором производятся вычисления.
Обычно хост это микропроцессор (CPU) так как исторически именно CPU имеет операционку с которой взаимодействует пользователь, который ставит задачи.
Устройством может быть видеокарточка (GPU), графический ускоритель, специализированная карта для машинного обучения (NPU), другие CPU и даже тот же самый CPU.

\subsection{Программирование GPU и GPGPU}\label{subsec:gpgpu}

Исторически первым опытом человечества в гетерогенном программировании были именно видеокарточки.
Созданные как суперпараллельные устройства с фиксированным графическим конвейером и фиксированным набором управляющих состоянием конвейера переключателей, видеокарточки довольно быстро превратились в нечто большее.
Первые короткие программы, позволяющие нетривиальную обработку вершин (после сборки геометрии) и фрагментов (после растеризатора) использовались в основном для освещения и поэтому назывались вершинными и фрагментными \textbf{шейдерами}.
Довольно скоро оказалось, что такие шейдеры полезны даже если пропустить собственно рендеринг: в параллельном стиле посчитать что-то на GPU систематически оказывалось быстрее и удобнее чем на CPU.

Поскольку шейдеры для графики изначально писались не как программы общего назначения а как специализированные программы для конкретных точек кастомизации, они писались не на языках общего назначения, а на специализированных языках из которых самыми популярными стали GLSL для OpenGL API и HLSL для DirectX API.

И OpenGL и DirectX и многие более поздние API являются именно API -- то есть набором вызовов, доступных программисту для совершения тех или иных действий.
Эти вызовы должны были где-то обрабатываться и формировать запросы к драйверу видеокарточки.
Так возникла концепция \textbf{рантайма}, как промежуточного слоя между пользовательской программой и драйвером.
Один и тот же рантайм может поддерживать несколько разных API и даже делегировать к нижележащим рантаймам.
Например в Intel как OpenCL API так и L0 API поддерживаются одним и тем же NEO runtime (который когда он работает с OpenCL API можно называть OpenCL runtime).

В число поддерживаемых вызовов API неизбежно входит вызов похожий на ``скомпилируй шейдер''. Таким образом неотъемлимой частью рантайма становится \textbf{графический компилятор}.

Первыми опытами в чисто вычислительных задачах исторически стали задачи свёртки (convolution). 
Именно концепция ядра свёртки (convolution kernel) дала название первым небольшим программам для гетерогенных вычислений: их стали называть \textbf{кернелами}.
Ниже везде использование терминов шейдер, кернел и программа будет полностью взаимозаменяемым.
В общем нет ошибки в том, чтобы назвать чисто вычислительную программу шейдером и так далее.

\subsection{О чём пойдёт речь далее}\label{subsec:about}

Структурно всё изложение ниже ведёт к глубокому анализу памяти и работы с ней в распространённых задачах гетерогенного программирования (см. раздел \ref{sec:FiveTasks}). Подготовленный читатель может сразу начать оттуда, но рекомендуется хотя бы ознакомиться с (\ref{subsec:memsend}) чтобы освежить варианты работы с памятью в SYCL.

На пути к этому будут изложены основы памяти для графических API (см. \ref{sec:GPUMEM}), мотивировано использование single-source API для compute (см. \ref{sec:SingleSource}) и дан краткий обзор компиляции и ассемблера для Intel XE (см. \ref{sec:IntelXE}).
Всё это должно помочь понять и воспроизвести соответствующие замеры, в том числе и на доступном читателю железе.
Даже не обязательно на железе Intel XE, так как общие концепции везде одинаковы.

Технический интерес могут также представлять концепции атомарности в мире обычного и гетерогенного программирования, к которым изложение придёт в конце, поскольку эти вопросы традиционно сложны.

\pagebreak
\section{Структура памяти для графики}\label{sec:GPUMEM}

Основные концепции в программировании графики примерно одинаковые но по разному называются для разных API. Ниже будут использоваться в основном термины из OpenGL и Vulkan -- из-за существенной тяги автора к открытым API. 

Важнейшей концепцией в программировании графики является память обладающая состоянием (stateful memory) и её разновидность: память с общей точкой привязки (bindless memory). 
Несмотря на то, что в языках, ориентированных на вычисления, таких концепций как будто нет, их понимание даёт важные инсайты для того чтобы далее понимать модель памяти, ракладку памяти и даже сгенерированный ассемблер.

\subsection{Память, обладающая состоянием}\label{subsec:stateful}

Ключевые типы памяти это uniform buffer object (UBO) и shared storage buffer object (SSBO). Простое объявление и того и другого проиллюстрировано на (lst. \ref{lst:glslbti}).

\begin{lstlisting}[caption={Простая программа на GLSL с явными binding индексами},label={lst:glslbti}]
struct Particle {
  vec4 pos, vel;
};

layout(std140, binding = 0) buffer Pos {
  Particle particles[];
};

layout (binding = 1) uniform UBO {
  float deltaT; 
  int count;
} ubo;
\end{lstlisting}

Следует обратить внимание на явное указание binding point для буфера. Обычно в GPU есть поддержка в железе для быстрого преобразования binding table index (BTI) и смещения в буфере в физический адрес. В данном случае программист вручную назначил точку с номером ноль для SSBO, а точку с номером один для UBO как показано на (рис. \ref{fig:bti-idea}).

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{pictures/bti-idea.pdf}
\caption{Binding table для (lst. \ref{lst:glslbti})}
\label{fig:bti-idea}
\end{figure}

После того как точки привязки назначены, работа с ними ведётся как с обычными массивами, структурами и даже массивами структур (lst. \ref{lst:glslprog}).

\begin{lstlisting}[caption={Работа с UBO и SSBO на GLSL},label={lst:glslprog}]
for (int i = 0; i < ubo.count; i += DataSz)
  if (i + x < ubo.count)
    someData[x] = particles[i + x].pos;
\end{lstlisting}

Но надо очень чётко осознавать: несмотря на то, что адресация \lstinline!particles[i + x]! выглядит похоже на работу с указателями в обычном C или C++, на самом деле это совсем другая концепция. Тут явно указывается буффер с его BTI и смещение в нём. Чтобы понять чем это отличается от настоящих указателей, давайте рассмотрим пример (lst. \ref{lst:glslbad})

\begin{lstlisting}[caption={Отсутствие настоящих указателей в GLSL},label={lst:glslbad}]
int *uboPtr = &ubo.deltaT; // this is not legal GLSL!
int *ssboPtr = &particles[i + x].pos; // and this is either
int *somePtr = CTUnknownCondition() ? ssboPtr : uboPtr;
\end{lstlisting}

Так делать в GLSL нельзя именно потому что наличие указателя как концепции позволяет смешать указатели в выражении, а это потеряет в \lstinline!someptr! информацию о том какой у него номер буфера и какой оффсет в нём.
В итоге компилятор будет просто не в состоянии построить ассемблер для выражения вроде \lstinline!*someptr = 8! так как останется неясно куда записывать восьмёрку.

Такая память, в которой обращение идёт опосредовано -- через BTI и смещение -- называется обладающей состоянием или stateful.
У вас есть незримое состояние: для каждого индекса BTI есть реальный физический адрес куда отображена память и размер буфера по этому адресу, а иногда и нечто другое.
Работа со stateful памятью за счёт аппаратной поддержки может быть очень эффективна, но фактически сырые (то есть stateless) указатели в такой модели оказываются запрещены: они позволяют слишком много.

\textbf{Обсуждение:} как вы думаете что делает stateful указатели более эффективными?
При ответе подумайте над размером stateless указателя в 64-битном адресном пространстве, над отсутствием поддержки 64-битной арифметики во многих видеокартах и над тем насколько эффективней заменить работу с 64-разрядными указателями на работу с 32-разрядными смещениями.

\textbf{Упражнение:} в этом подразделе использован известный пример N-body из
\href{https://github.com/SaschaWillems/Vulkan/blob/master/data/shaders/glsl/computenbody/particle_calculate.comp}{примеров Виллемса для Vulkan API}. Попробуйте скомпилировать его графическим компилятором вашего вендора графики и посмотрите получившийся ассемблер для вашей видеокарточки.

\subsection{Типизированная память}\label{subsec:typedmem}

Многие графические API работают со специальными объектами: изображениями и самплерами. Например (lst. \ref{lst:glslsamplers}) демонстрирует как выход растеризатора смешивается с текстурой и используется для получения итогового цвета фрагмента.

\begin{lstlisting}[caption={Работа с typed памятью},label={lst:glslsamplers}]
layout (binding = 0) uniform sampler2D samplerposition;

layout (location = 0) in vec2 inUV;
layout (location = 0) out vec4 outFragColor;

void main() {
  vec3 fragPos = texture(samplerposition, inUV).rgb;
  vec3 lightPos = vec3(0.0);
  vec3 L = normalize(lightPos - fragPos);
  float NdotL = max(0.5, dot(normal, L));
  outFragColor.rgb = NdotL;
}
\end{lstlisting}

Самое интересное тут это структура памяти текстуры. Очевидно это stateful память, но её state несколько сложнее.
Будучи изображением, текстура хранит свои внутренности в сложном формате, включающем размеры, pitch, цветовую схему.
Самплер как раз призван изолировать нас от этих деталей (а заодно сообщить железу как обрабатывать выходы за пределы текстуры и т.п. чтобы программист не писал этих условий руками).
Такая память, которая скрывает от нас сложные детали каждой своей ячейки, то есть имеет определенный тип ячейки, который снаружи виден как вектор, называется типизированной или \textbf{typed} памятью.

\subsection{Память с единой точкой привязки}\label{subsec:bindless}

Всего точек привязки обычно немного, что-то вроде 255 причём нижние две зарезервированы (например 254 это локальная память и 255 это \textbf{scratch} -- специальная память куда идут выгрузки данных не вместившихся в регистры).
При этом доступных дескрипторов для буферов в Vulkan API могут быть миллионы.

Ещё одна проблема в том, что массив текстур в этой модели нельзя индексировать неизвестным на этапе компиляции значением, так как обращение по оффсету может быть только в пределах одной точки привязки.

Из-за этого в программировании графики возникает концепция памяти с единой точки привязки (bindless memory), доступной, например в GLSL через расширение \lstinline!GL_EXT_nonuniform_qualifier!.

\begin{lstlisting}[caption={Работа с bindless памятью на GLSL},label={lst:bindless}]
#extension GL_EXT_nonuniform_qualifier : enable
layout (set = 1, binding = 10) uniform sampler2D textures[];
// ....
vec4 color = texture(textures[albedo_id], final_uv.xy);
\end{lstlisting}

Здесь \lstinline!albedo_id! может быть неизвестен на этапе компиляции, но это не так важно: весь массив, как показано на (рис. \ref{fig:bti-idea}) привязан к одной точке привязки и далее в пределах массива возможна индексация.
Эта концепция несколько приближает смещение в таком массиве к настоящему указателю, а сам массив -- к специальному адресному пространству к которому этот указатель принадлежит.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{pictures/bindless-idea.pdf}
\caption{Концепция bindless памяти для (lst. \ref{lst:bindless})}
\label{fig:bindless-idea}
\end{figure}

Главная претензия к отсутствию настоящих указателей: невозможность просто на оффсетах и буферах делать удобные дианмические структуры данных.
Конечно можно представить дерево или даже граф закодированные как линейный массив, но вряд ли операции над таким представлением будут сохранять асимптотическую эффективность.

\subsection{Конвергенция 3D: device pointers}\label{subsec:devpointers}

Графика постоянно приближается по мощности работы с памятью к вычислительным языкам, свидетельством чему является появившееся недавно расширение \lstinline!GLSL_EXT_buffer_reference!, работа с которым показана на (lst. \ref{lst:devptrs}).

\begin{lstlisting}[caption={Работа с device pointers на GLSL},label={lst:devptrs}]
// forward declaration
layout(buffer_reference) buffer blockType;

layout(buffer_reference, std430, buffer_reference_align = 16) 
buffer blockType {
  int x;
  blockType next;
};

layout(std430) buffer rootBlock {
  blockType root;
} r;

void main() {
  blockType b = r.root;
  // "pointer chasing" through a linked list
  b = b.next.next.next.next.next;
}
\end{lstlisting}

Тем не менее, даже такое совершенное приближение к концепции указателя не даёт ответа на фундаментальный вопрос: как быть с отображением указателей? 

Допустим мы находимся в мире гетерогенного программирования и у нас есть хост, который подготовил дерево, связный список или граф и куча устройств, которые готовы с этой структурой данных работать.
Можем ли мы прозрачно, указатель в указатель, отобразить эту структуру данных на устройство, не переделывая указатели в узлах?

То есть, шире говоря, возможно ли работать с памятью хоста и памятью одного или нескольких устройств как с унифицированной виртуальной памятью?

\pagebreak
\section{Вычислительные API}\label{sec:SingleSource}

Первые вычислительные API были разновидностью API для графики и до сих пор и OpenGL compute и Vulkan compute отлично существуют.
Их применимость ограничена их моделью памяти.
Кроме того они по уважительным причинам имеют чёткую границу между кодом для устройства и кодом для хоста.
Вы пишете хостовую программу и для неё код вашего шейдера это не код это кусок текста.
Это выглядит неплохо для графики, где шейдер не особо общается с хостовым кодом, а зависит в основном от выхода таких слабо контролируемых программистом устройств как сборщик геометрии или растеризатор.
Но что лучше подходит для вычислений?

\subsection{OpenCL}\label{subsec:opencl}

Появившийся в 2009-м году стандарт OpenCL (сравните с 1992-м годом OpenGL) ознаменовал начало эры программирования General Purpose GPU (или GPGPU), из которого потом вырос целый мир гетерогенного программирования.
Строго говоря, CUDA появилась в 2007-м, что несколько раньше, но, вспомним, что автор питает сильное пристрастие к открытым API, а CUDA сильно залочена на NVidia и имеет ряд других особенностей о которых ниже.
Будучи первым открытым compute API, OpenCL, разумеется, был сделан со всеми оглядками на существующие графические API, то есть separate-source. В общем и немудрено: семнадцать лет развития графики давали неплохую инерцию мышления.

Единицей работы с памятью в OpenCL является \textbf{буфер}.
Хостовая программа запрашивает у рантайма OpenCL создание буфера либо через явный вызов API, такой как clCreateBuffer, либо в более совершенном C++ API просто в конструкторе reference-counted RAII объекта.
Точно так же для пересылки есть явные API вроде clEnqueueReadBuffer, но, разумеется, лучше когда их просто вызывает под капотом алгоритм \lstinline!cl::copy!, как это показано на (lst. \ref{lst:oclbuf}).

\begin{lstlisting}[caption={Пересылка буфера, OpenCL},label={lst:oclbuf}]
cl::vector<int> A(BUFSZ), B(BUFSZ);
cl::Context Context{CL_DEVICE_TYPE_GPU, properties};
cl::CommandQueue Queue{Context};

// Buffer D on device
cl::Buffer D{Context, CL_MEM_READ_WRITE, BUFSZ * sizeof(int)};

// Send A to D (from host to device)
cl::copy(Queue, A.begin(), A.end(), D);

// Send D to B (from device back to host)
cl::copy(Queue, D, B.begin(), B.end());
\end{lstlisting}

С одной стороны буфер как идея напоминает нам BTI из графики. С другой стороны, простая программа на OpenCL C, которая работает с буферами, оперирует в качестве своих аргументов аннотированными указателями, как это показано на (lst. \ref{lst:oclvadd}).

\begin{lstlisting}[caption={Векторное сложение, OpenCL},label={lst:oclvadd}]
__kernel void
vector_add(__global int *A, __global int *B, __global int *C) {
  int i = get_global_id(0);
  C[i] = A[i] + B[i];
}
\end{lstlisting}

С точки зрения кода на C++ это просто текст и создание из него функтора происходит в рантайме.
Опять-таки можно использовать низкоуровневые вызовы clBuildProgram и т.п., можно проделать это более человечно в конструкторе RAII-объекта, как показано на (lst. \ref{lst:oclkern}).

\begin{lstlisting}[caption={Создание кернела, OpenCL},label={lst:oclkern}]
cl::Context Context{CL_DEVICE_TYPE_GPU, prop};
cl::CommandQueue Queue{Context};

// true == build immediately
cl::Program program{Context, vakernel, true};

cl::NDRange GlobalRange{Sz};
cl::EnqueueArgs Args{Queue, GlobalRange};
using KernTy = cl::KernelFunctor<cl::Buffer, cl::Buffer, cl::Buffer>;
KernTy add_vecs{program, "vector_add"};

// enque, execute, wait
cl::Event evt = add_vecs(Args, A, B, C);
\end{lstlisting}

На (рис. \ref{fig:oclkern}) показана схема того, что происходит с текстом программы когда он попадает в жернова рантайма и графического компилятора.
Программа обрабатывается фронтендом (часто clang-based) до стабильного промежуточного представления (стандарт де-факто это SPIRV, но могут быть варианты).
Далее подключается оптимизатор, который делает из бинарника в стабильном промежуточном представлении бинарник для конкретного устройства на котором исполняется программа.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{pictures/ocl-compilation-scheme.pdf}
\caption{Схема компиляции для (lst. \ref{lst:oclkern})}
\label{fig:oclkern}
\end{figure}

Разумеется не обязательно подавать текстовый файл и проходить все эти этапы.
OpenCL API позволяет вызывать и CreateProgramFromText и CreateProgramFromIL и даже CreateProgramFromBinary.
В последнем случае речь идёт об Ahead Of Time (AOT) компиляции, в первых двух случаях о разных версиях Just in Time (JIT) компиляции.
В случае AOT компиляции вы должны быть уверены что платформа и версия бинарного файла совпадут с вашим вычислительным устройством.

\subsection{CUDA, OpenMP, SYCL}\label{subsec:typesafety}

У API с раздельным исходным кодом есть фундаментальная проблема для вычислительных задач.
Поскольку они напрямую взаимодействуют с хостом (по сути они притворяются функциями, вызываемыми с хоста) у них есть границы интерфейсов.

Но хост не может проконтролировать безопасность типов по этим границам. Пример (lst. \ref{lst:oclvadd}) с точки зрения хостового кода выглядит как показано на (lst. \ref{lst:oclvaddhost}) то есть довольно-таки безнадёжно.

\begin{lstlisting}[caption={Векторное сложение, OpenCL c точки зрения хоста},label={lst:oclvaddhost}]
const char *vakernel = "__kernel void vector_add("
" __global int *A, __global int *B, __global int *C) {"
" int i = get_global_id(0);"
" C[i] = A[i] + B[i];"
"}";
\end{lstlisting}

\textbf{Обсуждение:} чему равен sizeof(int) на вашей графической карте и сколько вы будете отлаживать ошибку, если он не совпадёт с хостовым?

Разумеется есть много ухищрений на которые тут можно пойти, например использовать всюду \lstinline!cl_int!, для которого совпадение размеров гарантированно.
Но давайте будем честны: если у вас есть массив структур из двух int, вы вряд ли будете перекладывать его в массив структур из двух \lstinline!cl_int!.
Скорее уж вы будете надеяться что всё будет работать, у вас же на машине работает.
А даже если и переложите, то всё равно вам останется только молиться, чтобы совпало выравнивание.

Человечество осознало эту проблему довольно рано и первым single-source API стала CUDA в 2007-м году. После CUDA появился SYCL в 2014-м, и возможность оффлоада на гетерогенные устройства была добавлена в стандарт OpenMP.

Основной проблемой для single-source API является то, что обычный C++ не подходит для выражения ряда важных концепций. В нём нет разных типов памяти, нет интерфейсов для оффлоада и многого другого. Итак, что же можно сделать, чтобы решить эту проблему?

\textbf{Первый вариант:} расширить C++ новыми ключевыми словами и синтаксическими элементами. 

По этому пути изначально пошла CUDA, см. (lst. \ref{lst:cudavadd}) и он кажется наиболее простым.
Но на самом деле он порождает массу сложностей: если мы пишем не вполне на C++, насколько, собственно идиомы и соглашения C++ приложимы к тому, на чём мы пишем?
Как оно будет эволюционировать с новыми стандартами?
Можем ли мы судить о поведении всей программы, например при ODR violation для кернелов?

\begin{lstlisting}[caption={Векторное сложение, CUDA},label={lst:cudavadd}]
__global__ void vecAdd(int *A, int *B, int *C, int N);
....
cudaMalloc((void **)&a_d, n * sizeof(int));
cudaMalloc((void **)&b_d, n * sizeof(int));
cudaMalloc((void **)&c_d, n * sizeof(int));
vecAdd<<<block_no, block_size>>>(a_d, b_d, c_d, n);
\end{lstlisting}

Конечно, если рано или поздно это удастся протащить в стандарт, эти вопросы будут сняты, но пока что кажется до этого далеко.
Да и корпорация NVidia, судя по их лицензии, явно не хочет видеть альтернативных реализаций CUDA.

\textbf{Второй вариант:} расширить C++ новыми прагмами.

Этот путь выбрали в OpenMP, для примера см. (lst. \ref{lst:ompred}).
Причём этот выбор был сделан задолго до привнесения в OpenMP самой возможности делать оффлоады.
Оно изначально было так спроектировано.

\begin{lstlisting}[caption={Редукция, OpenMP},label={lst:ompred}]
#pragma omp target parallel private(ompSum) shared(totalSum)
{
  ompSum = 0;

  #pragma omp for
  for (int i = 0; i < N; i++)
    ompSum += array[i];

  #pragma omp critical
  totalSum += ompSum;
}
\end{lstlisting}

Здесь те же возражения в несколько ослабленном виде.
Являясь нестандартными конструкциями, прагмы притворяются что их нет.
Но объекты, на которые они влияют, нетривиально взаимодействуют с объектами, подчиняющимися стандарту.
Возьмём прагму \lstinline!shared(totalSum)! -- она как минимум изменяет область видимости, время жизни и правила synchronized-with для аннотируемой переменной.
Язык C++ достаточно сложен сам по себе, чтобы не перегружать его дополнительной семантикой такого рода.
Кроме того субъективно это критично ухудшает читаемость кода (впрочем это не аргумент).

\textbf{Третий вариант:} не трогать C++, обойтись библиотечной поддержкой.

Это путь по которому пошёл, например, SYCL см. (lst. \ref{lst:syclfn}).
Ключевая развязка хостового кода с кодом устройства осуществляется через взятую из OpenCL концепцию очереди.

\begin{lstlisting}[caption={Выполнение DeviceFn, SYCL},label={lst:syclfn}]
auto Event = Queue.submit(
  [&](sycl::handler &Cgh) {
    // .....
    Cgh.parallel_for<name>(Range, DeviceFn);
  });
\end{lstlisting}

Но в очередь ставится не конкретная функция, а command group: кернел плюс его зависимости. На (рис. \ref{fig:syclfn}) показано как это может происходить.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{pictures/hetero-prog-queue.pdf}
\caption{Схема постановки в очередь для (lst. \ref{lst:syclfn})}
\label{fig:syclfn}
\end{figure}

Устройства, имеющие один и тот же компилятор, один и тот же бинарный формат (например два GPU одного вендора) могут быть объединены в контекст. Простейший контекст это единственное устройство. Далее кернел, скомпилированный в контексте может быть поставлен в очередь на одно из его устройств.
При этом поддерживается сквозная типизация, что уберегает от ошибок из-за несогласованности размеров и прочего.

\subsection{Несогласованность имён}\label{subsec:names}

В разных вычислительных API используются разные названия для разных логических видов памяти. Общая схема именований разной памяти, принятая в OpenCL и в SYCL показана на (рис. \ref{fig:logicalmemory}).

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{pictures/logical-memory.pdf}
\caption{Логическая схема памяти в OpenCL и SYCL}
\label{fig:logicalmemory}
\end{figure}

Но надо понимать, что в той же CUDA:
\begin{itemize}
\item \textbf{local} это то, что в SYCL называется private
\item \textbf{shared} это то, что в SYCL называется local (или shared local)
\item \textbf{unified} это то, что в SYCL называется shared (или unified shared)
\item \textbf{device} это то, что в SYCL называется global или device.
\end{itemize}

В этом довольно легко запутаться. Далее для всех примеров будет использоваться SYCL (и соглашения об именованиях SYCL).

\subsection{Память хоста и память устройств в SYCL}\label{subsec:memsend}

В модели SYCL есть три основных способа пересылать память с хоста на устройство и обратно: использование аксессоров, использование общей памяти и непосредственная работа с памятью устройства.
Все три способа надо хорошо знать, поэтому имеет смысл расмсотреть все.

\textbf{Вариант 1: использование аксессоров}

Аксессор это объект, действующий как массив в том адресном пространстве, которым он параметризован.
По умолчанию это массив в глобальной памяти устройства.
Поскольку в языке C++ нет понятия адресного пространства, аксессоры очень полезны, так как вводят эту концепцию на уровне библиотеки.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{pictures/accessors.pdf}
\caption{Идея аксессора}
\label{fig:accessors}
\end{figure}

Идея аксессора показана на (рис. \ref{fig:accessors}), а код, использующий аксессор приведен на (lst. \ref{lst:accessors}).

\begin{lstlisting}[caption={Использование аксессора},label={lst:accessors}]
std::vector<T> AH = /* AX * AY matrix data */;

// use buffer to send on device
sycl::buffer<T, 2> BufA(AH.data(), AH.size());

Queue.submit([&](sycl::handler &Cgh) { 
  auto A = BufA.get_access<sycl_read>(Cgh);
  // .....
  auto Kernel = [=](sycl::id<2> Id) {
    int Row = Id.get(0), Col = Id.get(1);
    // use A[Row][Col] here
\end{lstlisting}

Тут следует обратить внимание, что аксессор может быть многомерным (например в примере lst. \ref{lst:accessors} он двумерный) и тогда он ведет себя как многомерный массив.

\textbf{Вариант 2: общая память}

Идея общей виртуальной памяти между устройством и хостом с общими адресами в едином адресном пространстве -- немыслима для графики.
Но для вычислений это иногда ровно то, что нужно.
Основная идея проиллюстрирована на (рис. \ref{fig:sharedmem}).

\begin{lstlisting}[caption={Использование общей памяти},label={lst:sharedmem}]
std::vector<T> AH = /* AX * AY matrix data */;

// allocate shared memory and copy to device
T *A = malloc_shared<T>(AX * AY, Queue);
std::copy(AH.begin(), AH.end(), A);

Queue.submit([&](sycl::handler &Cgh) { 
  auto Kernel = [=](sycl::id<2> Id) {
    int Row = Id.get(0), Col = Id.get(1);
    // use A[Row * AY + Col] here
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{pictures/sycl-device-and-shared-mem-v.pdf}
\caption{Аксессор и общая память}
\label{fig:sharedmem}
\end{figure}

Разумеется ручное управление через \lstinline!malloc_shared! выглядит немного наивно.
Мы с вами пишем на C++ и, конечно, хотели бы использовать аллокаторы. К счастью это возможно.

\begin{lstlisting}[caption={Аллокатор в общей памяти},label={lst:sharedalloc}]
using AllocTy = usm_allocator<int, sycl::usm::alloc::shared>;
AllocTy Alloc{DeviceQueue};
std::vector<int, AllocTy> AData(AX * AY, Alloc);
\end{lstlisting}

Теперь память за программиста синхронизирует железо и управление памятью отдано контейнеру.
Это отлично, но это немного расслабляет, иногда хочется очень точного контроля над происходящим.

\textbf{Вариант 3: явное управление памятью}

Этот вариант больше всего похож на OpenCL с его системой явного копирования (см. lst. \ref{lst:oclbuf}), только здесь, как показано на (lst. \ref{lst:devicemem}) копирование это метод в классе очереди.

Вызов функции для выделения памяти устройства производится с хоста, но это не должно вводить в заблуждение: в конце концов clCreateBuffer тоже делается с хоста.
Вообще автору кажется, что тот факт, что только хост может общаться с рантаймом, рано или поздно должен быть пересмотрен.

\begin{lstlisting}[caption={Явная пересылка на устройство},label={lst:devicemem}]
std::vector<T> AH;
.....
T *A = malloc_device<T>(AX * AY, Queue);
auto EvtCpyData = Queue.copy(AH.data(), A, AH.size());
.....
Queue.submit([&](sycl::handler &Cgh) { 
  Cgh.depends_on(EvtCpyData);
  auto Kernel = [=](sycl::id<2> Id) {
    // use A[Row * AY + Col] here
\end{lstlisting}

Попытка обратиться в память устройства с хоста приведет к ошибке времени выполнения.

\textbf{Упражнение:} реализуйте сложение векторов на вашей системе всеми тремя способами. Измерьте по таймингам эвентов затраченное время.

Главная задача дальнейшего изложения это показать как память в гетерогенных системах соотносится с обычной памятью в обычных программах на C++.
Проще всего это достигается при рассмотрении конкретных примеров.
Но чтобы рассматривать конкретные примеры надо договориться о конкретной архитектуре.

\pagebreak
\section{Компиляция OneAPI и ассемблер Intel XE}\label{sec:IntelXE}

Сейчас самый удобный способ писать на SYCL это использование Intel OneAPI.
Несмотря на то что производительность (зачастую встроенных) видеокарт Intel пока что отстаёт от лучших образцов дискретной графики NVidia, они широко распространены и многие задачи на них удивительно эффективны (особенно после 12-го поколения).
Бэкенды SYCL для других видеокарт (в том числе NVidia и AMD) существуют, но проигрывают специализированным средствам.
А для Intel, компилятор SYCL из поставки OneAPI это и есть специализированное средство, максимально оптимизируемое под доступное железо.

Для того, чтобы исследовать ассемблер и поэтапную компиляцию, лучше использовать какой-нибудь очень простой пример.
Классический пример не очень полезной, но максимально простой задачи это сложение векторов (см. \ref{lst:syclvadd}).

\begin{lstlisting}[caption={Сложение векторов на SYCL},label={lst:syclvadd}]
// allocate shared memory, common for device and host
auto *A = cl::sycl::malloc_shared<T>(Sz, Queue);
auto *B = cl::sycl::malloc_shared<T>(Sz, Queue);
auto *C = cl::sycl::malloc_shared<T>(Sz, Queue);

// copy to shared memory
std::copy(AVec, AVec + Sz, A);
std::copy(BVec, BVec + Sz, B);

// offload and wait (here GPU computation may happen)
Queue.parallel_for(cl::sycl::range<1>{Sz},
                   [=](auto n) { C[n] = A[n] + B[n]; });
Queue.wait();

// copy back from shared memory
std::copy(C, C + Sz, CVec);

// free shared memory 
cl::sycl::free(A, Queue);
cl::sycl::free(B, Queue);
cl::sycl::free(C, Queue);
\end{lstlisting}

Схема компиляции включает:

\begin{itemize}
\item \textbf{SYCL compiler}, который разделяет исходники на код для хоста и устройства и компилирует код устройства до стабильного промежуточного представления (SPIRV).
\item \textbf{Graphics compiler}, который конвертирует SPIRV в LLVM IR и оптимизирует IR до генерации бинарника устройства. 
\item \textbf{Compute Runtime}, который взаимодействует с OpenCL API, устанавливает значения аргументов, включая неявные и более того.
\end{itemize}

\subsection{От исходников до SPIRV}\label{subsec:syclcompiler}

Результатом работы компилятора SYCL является хостовый бинарник с секциями SPIRV. Для этого, как показано на (рис. \ref{fig:syclscheme}) компилятор должен разделить единый исходник на часть с кодом для хоста и часть с кодом для устройства.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{pictures/sycl-scheme.pdf}
\caption{Схема компиляции до бинарника со SPIRV секциями}
\label{fig:syclscheme}
\end{figure}

LLVM IR для устройства имеет узнаваемый паттерн векторного сложения.

\begin{lstlisting}[language=llvm,caption={Сложение векторов: device IR после SYCL компилятора},label={lst:syclvaddllvm}]
%8 = getelementptr inbounds i32, i32 addrspace(1)* %1, i64 %5
%9 = addrspacecast i32 addrspace(1)* %8 to i32 addrspace(4)*
%10 = load i32, i32 addrspace(4)* %9, align 4
%11 = getelementptr inbounds i32, i32 addrspace(1)* %2, i64 %5
%12 = addrspacecast i32 addrspace(1)* %11 to i32 addrspace(4)*
%13 = load i32, i32 addrspace(4)* %12, align 4
%14 = add nsw i32 %10, %13
%15 = getelementptr inbounds i32, i32 addrspace(1)* %0, i64 %5
%16 = addrspacecast i32 addrspace(1)* %15 to i32 addrspace(4)*
store i32 %14, i32 addrspace(4)* %16, align 4
\end{lstlisting}

Все загрузки делаются из \lstinline!addrspace(4)! это generic память (то есть память о которой мы ничего не можем сказать) поскольку компилятор SYCL консервативен и оставляет разрешение адресного пространства следующему за ним оптимизатору.

\subsection{От SPIRV до VISA}\label{subsec:igc}

После оптимизации графическим компилятором, включающей в том числе stateless-to-stateful преобразование, LLVM IR начинает выглядеть следующим образом.

\begin{lstlisting}[language=llvm,caption={Сложение векторов: device IR после компилятора IGC},label={lst:syclvaddigc}]
%5 = inttoptr i32 %4 to i32 addrspace(131073)*
%6 = load i32, i32 addrspace(131073)* %5, align 4
%7 = inttoptr i32 %4 to i32 addrspace(131074)*
%8 = load i32, i32 addrspace(131074)* %7, align 4
%9 = add nsw i32 %6, %8
%10 = inttoptr i32 %4 to i32 addrspace(131072)*
store i32 %9, i32 addrspace(131072)* %10, align 4
\end{lstlisting}

Обратите внимание на странную конструкцию \lstinline!i32 addrspace(131073)*!. По сути это та самая stateful память о которой шла речь в (\ref{subsec:stateful}).

Далее полученное представление векторизуется и из него строится виртуальный ассемблер, который затем финализируется до физического (но везде ниже будет нужен только виртуальный).

Виртуальный ассемблер для Intel XE задокументирован по ссылке:
\href{https://github.com/intel/intel-graphics-compiler/blob/master/documentation/visa/1_introduction.md}{VISA introduction}

Его главными особенностями являются: произвольное количество векторных регистров, не зависящие от архитектуры операции, символическое представление операций.

\begin{lstlisting}[language={[x86masm]Assembler},caption={Сложение векторов: VISA ассемблер},label={lst:syclvaddvisa}]
movs (M1_NM, 1) T6(0) 0x1:ud
gather4_scaled.R (M1, 16) T6 0x0:ud V0054.0 V0056.0
movs (M1_NM, 1) T6(0) 0x1:ud
gather4_scaled.R (M5, 16) T6 0x0:ud V0055.0 V0057.0
movs (M1_NM, 1) T6(0) 0x2:ud
gather4_scaled.R (M1, 16) T6 0x0:ud V0054.0 V0058.0
movs (M1_NM, 1) T6(0) 0x2:ud
gather4_scaled.R (M5, 16) T6 0x0:ud V0055.0 V0059.0
add (M1, 16) V0056(0,0)<1> V0056(0,0)<1;1,0> V0058(0,0)<1;1,0>
add (M5, 16) V0057(0,0)<1> V0057(0,0)<1;1,0> V0059(0,0)<1;1,0>
movs (M1_NM, 1) T6(0) 0x0:ud
scatter4_scaled.R (M1, 16) T6 0x0:ud V0054.0 V0056.0
movs (M1_NM, 1) T6(0) 0x0:ud
scatter4_scaled.R (M5, 16) T6 0x0:ud V0055.0 V0057.0
\end{lstlisting}

Разберем для примера синтаксис \lstinline!gather4_scaled!. Это загрузка из stateful памяти которая берет аргументами BTI index, вектор смещений и заполняет регистр, указанный последним аргументом, значениями из памяти.
Указание \lstinline!(M1, 16)! означает векторную операцию SIMD16, действующую на линиях с нулевой по пятнадцатую (инструкция также учитывает глобальную маску, которая может отключить дополнительные линии).
Аналогично \lstinline!(M5, 16)! означает векторную операцию SIMD16, действующую на линиях с пятнадцатой по тридцать первую.
Регистр \lstinline!T6! хранит BTI индекс.
Константа \lstinline!0x0:ud! это база для чтения
Регистр \lstinline!V0054! хранит вектор из 16-ти смещений, а регистр \lstinline!V0056! является регистром назначения.

\textbf{Обсуждение:} компилятор делает stateless-to-stateful преобразование если видит, что указатель внутри кернела никто не использует сложнее чем индекс буфера плюс оффсет. Попробуйте его сломать: то есть напишите vector addition так чтобы это преобразование не применилось. Посмотрите какой ассемблер вы получите.

\pagebreak
\section{Пять главных задач на SYCL}\label{sec:FiveTasks}

Пятью главными базовыми задачами, которые решаются на видеокарточках являются:

\begin{enumerate}
\item Перемножение матриц
\item Сортировка
\item Свёртки
\item Редукция
\item Гистограмма
\end{enumerate}

Разумеется на видеокарточках решается масса более сложных задач, но, если к ним приглядеться, они обычно распадаются на эти составляющие.
Например что такое inference в сверточной нейросети?
Это перемножение матриц плюс свёртка плюс редукция.

Разумеется начать обсуждение главных задач следует с перемножения матриц.
Для GPGPU это задача задач и выполняя её хорошо, можно сделать очень и очень много практически важных вещей.

\subsection{Перемножение матриц (приватная память)}\label{subsec:gemm}

На (рис. \ref{fig:iterspace}) изображена схема итерационного пространства при умножении матрицы размерами \lstinline!AX * AY! на матрицу, размерами \lstinline!AY * BY!. Каждый поток обрабатывает один элемент итерационного пространства, размером \lstinline!AX * BY!.

Традиционно в перемножении матриц важен ведущий размер, и, как показано на (рис. \ref{fig:iterspace}) именно ведущий размер обозначается за размер X: например в AX или в BX (но вместо него используется AY).
Это немного контринтуитивно, но к этому можно быстро привыкнуть.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{pictures/iter-space-matrix-single.pdf}
\caption{Схема итерационного пространства при перемножении матриц}
\label{fig:iterspace}
\end{figure}

Максимально наивное перемножение приведено на (lst. \ref{lst:gemmnoprivmem}). Здесь написано ровно то, что изображено на рисунке: каждый следующий элемент матрицы \lstinline!C[Row][Col]! формируется отдельным потоком как сумма произведений строки и столбца исходных матриц.

\begin{lstlisting}[caption={Наивное перемножение матриц без приватной памяти},label={lst:gemmnoprivmem}]
Queue.submit([&](sycl::handler &Cgh) {
  // .....
  auto K = [=](sycl::id<2> Id) {
    const int Row = Id.get(0);
    const int Col = Id.get(1);
    for (int K = 0; K < AY; K++)
      C[Row][Col] += A[Row][K] * B[K][Col];     
  };
  Cgh.parallel_for<class m<T>>({AX, BY}, K);
}
\end{lstlisting}

Здесь можно задаться вопросом: а какое адресное пространство у переменных \lstinline!Row!, \lstinline!Col! и \lstinline!K!?
Будь эти переменные переменнымив  обычной программе на CPU, они были бы выделены на регистрах.
В программе на GPU они находятся в \textbf{приватной} памяти.
Она уже вскользь упоминалась на схеме (рис. \ref{fig:logicalmemory})
Идея в большем разрешении показана на (рис. \ref{fig:privatemem}).

\begin{figure}
\centering
\includegraphics[width=0.3\textwidth]{pictures/sycl-privatemem-shorter.pdf}
\caption{Идея приватной памяти}
\label{fig:privatemem}
\end{figure}

Своё небольшое количество приватной памяти доступно каждому логчиескому потоку.
Это даёт идею переписать код, используя аккумулятор в приватной памяти, как показано на (lst. \ref{lst:gemmprivmem}).

\begin{lstlisting}[caption={Перемножение матриц с использованием приватной памяти},label={lst:gemmprivmem}]
T Sum = 0;
for (int K = 0; K < AY; K++)
  Sum += A[Row][K] * B[K][Col];
C[Row][Col] = Sum;
\end{lstlisting}

Разумеется, внимательный читатель имеет основания задаться вопросом: зачем вообще писать умножение матриц с нуля, неужели нет доступной библиотечной реализации?
В случае SYCL стандартной библиотеки нет, но есть Intel oneMKL, построенная на основе библиотеки Blas и поддерживающая близкий к ней интерфейс.
Реализация перемножения с использованием этой библиотеки (собственно в один вызов) показана на (lst. \ref{lst:gemmmkl}).

\begin{lstlisting}[caption={Перемножение матриц с использованием библиотеки MKL},label={lst:gemmmkl}]
auto *A = sycl::malloc_shared<T>(AX * AY, DeviceQueue);
auto *B = sycl::malloc_shared<T>(AY * BY, DeviceQueue);
auto *C = sycl::malloc_shared<T>(AX * BY, DeviceQueue);
std::copy(Aptr, Aptr + AX * AY, A);
std::copy(Bptr, Bptr + AY * BY, B);

// multiplication with MKL library
std::vector<sycl::event> GemmDependencies;
float Alpha = 1.0f, Beta = 0.0f;
mkl::transpose TransA = mkl::transpose::nontrans;
mkl::transpose TransB = mkl::transpose::nontrans;
mkl::blas::gemm(DeviceQueue, TransA, TransB, AX, BY, AY,
                Alpha, A, AX, B, AY, Beta, C, AX, GemmDependencies);
\end{lstlisting}

\textbf{Обсуждение:} какие недоcтатки вы видите в использовании oneMKL?
При ответе подумайте о том, что она существует по сути только для двух типов: float и double.
Даже перемножение целочисленных матриц с использованием этой библиотеки будет проблемой.

Результаты замеров самописного произведения с приватной памятью, без неё и с использованием OneMKL показаны на (рис. \ref{fig:sgemm_priv}). Видно, что использование приватной памяти делает ситуацию в разы лучше (и вровень с MKL), а попытка без неё обойтись приводит к очень печальному эффекту.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{pictures/sgemm_priv.pdf}
\caption{Замеры, показывающие эффект приватной памяти на TGLLP}
\label{fig:sgemm_priv}
\end{figure}

Как объяснить эти замеры и за счёт чего происходит такая просадка? 
Многое становится ясным, если посмотреть виртуальный ассемблер для Intel XE (см. \ref{subsec:igc}), полученный в обоих случаях. Сравните (lst. \ref{lst:visapriv}) и (lst. \ref{lst:visanopriv}).
В обоих случаях очевиден SIMD32 dispatch, но обратная дуга идёт совершенно по разному.

\begin{lstlisting}[language={[x86masm]Assembler},caption={VISA ассемблер: счётчик в приватной памяти},label={lst:visapriv}]
label5:
  ...
  gather4_scaled.R (M1, 16) T6 0x0:ud V192.0 V194.0
  gather4_scaled.R (M5, 16) T6 0x0:ud V193.0 V195.0
  mad (M1, 16) V116(0,0)<1> V194(0,0)<1;1,0>
               V175(0,0)<1;1,0> V116(0,0)<1;1,0>
  mad (M5, 16) V117(0,0)<1> V195(0,0)<1;1,0>
               V176(0,0)<1;1,0> V117(0,0)<1;1,0>
  ...
  goto (M1, 1) label5
  ...
  scatter4_scaled.R (M1, 16) T6 0x0:ud V227.0 V116.0
  scatter4_scaled.R (M5, 16) T6 0x0:ud V228.0 V117.0
\end{lstlisting}

В случае использования приватной памяти накопление суммы произведений происходит в регистрах \lstinline!V116! и \lstinline!V117!, которые сохраняются в память после завершения цикла. Это делает всего две записи в память на цикл.

\begin{lstlisting}[language={[x86masm]Assembler},caption={VISA ассемблер: приватная память не использована},label={lst:visanopriv}]
label3:
  ...
  gather4_scaled.R (M1, 16) T6 0x0:ud V229.0 V264.0
  gather4_scaled.R (M5, 16) T6 0x0:ud V230.0 V265.0
  mad (M1, 16) V264(0,0)<1> V262(0,0)<1;1,0>
               V237(0,0)<1;1,0> V264(0,0)<1;1,0>
  mad (M5, 16) V265(0,0)<1> V263(0,0)<1;1,0>
               V238(0,0)<1;1,0> V265(0,0)<1;1,0>
  scatter4_scaled.R (M1, 16) T6 0x0:ud V229.0 V264.0
  scatter4_scaled.R (M5, 16) T6 0x0:ud V230.0 V265.0
  ....
  goto (M1, 1) label3
\end{lstlisting}

Без использования приватной памяти, накопление осуществляется сразу в памяти и, как видно из (lst. \ref{lst:visanopriv}) происходит немыслимое количество ненужных операций записи в память.

\textbf{Обсуждение:} как вы думаете можно ли это соптимизировать графическим компилятором? Не торопитесь с ответом на этот вопрос, сначала внимательно прочитайте текст на эту тему \href{https://llvm.org/docs/Atomics.html#id5}{на llvm.org}.

Ещё одна идея, которая приходит в голову каждому человеку, знакомому с оптимизацией умножения матриц на CPU это транспонировать одну из матриц чтобы бежать во вложенном цикле по одному и тому же индексу, как это показано на (lst. \ref{lst:gemmtrans}).

\begin{lstlisting}[caption={Перемножение матриц с предварительным транспонированием},label={lst:gemmtrans}]
std::copy(Aptr, Aptr + AX * AY, A);
for (int i = 0; i < AY; ++i)
  for (int j = 0; j < BY; ++j)
    B[j * AY + i] = Bptr[i * BY + j];
// .....
Queue.submit([&](sycl::handler &Cgh) {
  // .....
  auto K = [=](sycl::id<2> Id) {
    const int Row = Id.get(0);
    const int Col = Id.get(1);
    for (int K = 0; K < AY; K++)
      Sum += A[Row * AY + K] * B[Col * AY + K];
    C[Row * BY + Col] = Sum;
  };
  Cgh.parallel_for<class m<T>>({AX, BY}, K);
}
\end{lstlisting}

Можно также сделать транспонирование на MKL.
Достаточно точно также как на (lst. \ref{lst:gemmtrans}) провести его в (lst. \ref{lst:gemmmkl}) вручную или специальной вспомогательной функцией до вызова \lstinline!mkl::blas::gemm! и информировать об этом библиотеку установкой \lstinline!TransB!.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{pictures/sgemm_trans.pdf}
\caption{Замеры, показывающие негативный эффект транспонирования на TGLLP}
\label{fig:sgemm_trans}
\end{figure}

Результаты замеров показаны на (рис. \ref{fig:sgemm_trans}). Видно, что для транспонирования проведенного вручную они крайне негативны и причина этому -- расположение памяти в буфере. Интуиция с кешами CPU тут не работает, так как на видеокарте, если вы явно не попросите, обычно вообще нет кешей.

\textbf{Обсуждение:} почему в случае OneMKL транспонирование не даёт отрицательного эффекта? Для ответа вам может понадобиться заглянуть как в исходники, так и в сгенерированный ассемблер.

\subsection{Перемножение матриц: вложенный параллелизм}\label{subsec:gemmnested}

Локальная память очень быстрая, но обычно её очень мало. Она делается из Static RAM -- это тот же материал из которого делаются кеши и одна её ячейка в разы дороже и потребляет в разы больше энергии чем ячейка Dynamic RAM.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{pictures/iter-space-matrix-cache.pdf}
\caption{Использование локальной памяти в перемножении матриц}
\label{fig:gemmlocal}
\end{figure}

На (рис. \ref{fig:gemmlocal}) показана основная идея: совсем небольшой участок, также называемый tile (тайл) используется для обоих матриц для того чтобы идти по строчкам и столбцам и делать самые дорогие операции над локальной памятью.
На TGLLP тайл это \lstinline!16 * 16! элементов т.к. больше локальное итерационное пространство просто не позволяет.
Часть входных матриц копируется в этот тайл и далее после произведения операций, обновляется глобальная память результата.

Можно начать опыты по использованию локальной памяти с явного (также известного как вложенного, nested) параллелизма, который поддержан в SYCL.
Пример его использования приведён на (lst. \ref{lst:gemmnested}).

\begin{lstlisting}[caption={Перемножение матриц, явный параллелизм},label={lst:gemmnested}]
using LTy = sycl::accessor<T, 2, sycl_read_write, sycl_local>;
LTy Asub{BlockSize, Cgh}, Bsub{BlockSize, Cgh};

auto KernMul = [=](sycl::group<2> Group) {
  sycl::private_memory<int, 2> Sum(Group);

  Group.parallel_for_work_item([&](sycl::h_item<2> It) { 
    Sum(It) = 0;
  });

  for (int Tile = 0; Tile < NumTiles; Tile++) {
    Group.parallel_for_work_item([&](sycl::h_item<2> It) {
      int GlobalRow = It.get_global_id(0);
      int GlobalCol = It.get_global_id(1);
      int Row = It.get_local_id(0);
      int Col = It.get_local_id(1);
      int TiledRow = LSZ * Tile + Row;
      int TiledCol = LSZ * Tile + Col;

      Asub[Row][Col] = A[GlobalRow][TiledCol];
      Bsub[Row][Col] = B[TiledRow][GlobalCol];
    });

    Group.parallel_for_work_item([&](sycl::h_item<2> It) {
      int Row = It.get_local_id(0);
      int Col = It.get_local_id(1);

      for (int K = 0; K < LSZ; K++)
        Sum(It) += Asub[Row][K] * Bsub[K][Col];
    });
  }

  Group.parallel_for_work_item([&](sycl::h_item<2> It) {
    int GlobalRow = It.get_global_id(0);
    int GlobalCol = It.get_global_id(1);
    C[GlobalRow][GlobalCol] = Sum(It);
  });
};

Cgh.parallel_for_work_group<class mmult_groups_priv<T>>(
    NumGroups, BlockSize, KernMul);
\end{lstlisting}

Теперь кернел явно разбит на два уровня вложенных параллельных циклов: верхний \lstinline!parallel_for_work_group! и нижний уровень. Последний представлен несколькими \lstinline!parallel_for_work_item!.
Специальная конструкция \lstinline!sycl::private_memory<int, 2>! нужна для того, чтобы информировать графический компилятор об использовании приватной памяти.
В отличии от обычного \lstinline!parallel_for!, где приватная память это просто локальная переменная (как на lst. \ref{lst:gemmprivmem}), внутри специальной конструкции \lstinline!parallel_for_work_item! локальная переменная будет переменной в локальной памяти.

Результаты этого опыта показаны на (рис. \ref{fig:sgemm_groups}) и их можно назвать удручающими.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{pictures/sgemm_groups.pdf}
\caption{Замеры, показывающие проблемы явного параллелизма на TGLLP}
\label{fig:sgemm_groups}
\end{figure}

Во многом проблемы явного параллелизма это проблемы компилятора, который в данном случае должен сделать слишком много и не делает практически ничего.
В частности компилятор должен расставить барьеры, поскольку рабочая группа, совместно работающая с локальной памятью это элемент коорперативной многозадачности.
Первый же взгляд на visaasm показывает довольно плохой сгенерированный код.
Есть надежда, что он будет существенно улучшен в следующих версиях OneAPI.

\subsection{Разворот гнезда циклов}\label{subsec:gemmtransform}

В общем случае идея перехода от гнезда циклов с самым вложенным \lstinline!parallel_for! к такому гнезду, где \lstinline!parallel_for! сверху в условиях когда присутствует локальная память делается как показано на (lst. \ref{lst:transition}).

\begin{lstlisting}[caption={Разворот гнезда циклов},label={lst:transition}]
for (auto i : Ni)
  for (auto j : Nj)    
    parallel for (auto k : Nk)
      do(i, j, k); 

// given Nk is small may be transformed to:

parallel for (auto k : Nk)
  for (auto i : Ni)
    for (auto j : Nj) {
      do(i, j, k); 

      // wait all workitems
      barrier(k);
    }
\end{lstlisting}

Можно применить эту общую идею (она ещё будет нужна в битнической сортировке и далее) к циклу на (lst. \ref{lst:gemmnest}).
Но лучше прежде немного упростить идею и показать этот цикл псевдокодом.
Стартовая точка разворота это

\begin{lstlisting}[caption={Гнездо циклов для GEMM},label={lst:gemmnest}]
private S = 0;
for (Tile = 0; Tile < NumTiles; ++Tile) {
  parallel for(auto Wi: Workitems)
    copy_global_to_local(Tile, Wi); 
  parallel for(auto Wi: Workitems)
    S = calculate_local_mmult(wi);
}
parallel for(auto Wi: Workitems)
  update_global_memory(S, Wi); 
\end{lstlisting}

Чисто формально применяя идею (lst. \ref{lst:transition}), из (lst. \ref{lst:gemmnest}) можно получить (lst. \ref{lst:gemmnest2}).

\begin{lstlisting}[caption={Формальное преобразование циклов для GEMM},label={lst:gemmnest2}]
parallel for(auto Wi: Workitems)
  for (Tile = 0; Tile < NumTiles; ++Tile) {
    copy_global_to_local(Tile, Wi);
    barrier;
  }

parallel for(auto Wi: Workitems)
  for (Tile = 0; Tile < NumTiles; ++Tile) {
    S = calculate_local_mmult(wi);
    barrier;
  }

parallel for(auto Wi: Workitems)
  update_global_memory(S, Wi); 
\end{lstlisting}

Пока что особой пользы не видно.
Но тут и пряовляется волшебная сила барьера как примитива синхронизации.
Поскольку далее, так как внутри разных параллельных посылок на барьере ожидается итерация фактически одного и того же цикла по тайлам, циклы можно смёржить, получив (lst. \ref{lst:gemmnest3}).

\begin{lstlisting}[caption={Итоговое преобразование циклов для GEMM},label={lst:gemmnest3}]
parallel for(auto Wi: Workitems) {
  private S = 0;
  for (Tile = 0; Tile < NumTiles; ++Tile) {
    copy_global_to_local(Tile, Wi);
    barrier;
    S = calculate_local_mmult(wi);
    barrier
  }
  update_global_memory(S, Wi);
}
\end{lstlisting}

Эта техника очень полезна и, как станет ясно далее, очень практична.

\textbf{Обсуждение:} почему мы можем надеяться что такое преобразование гнезда циклов поможет производительности?

\subsection{Перемножение матриц: локальная память}\label{subsec:gemmlocal}

Псевдокод (lst. \ref{lst:gemmnest3}) можно переписать настоящим кодом, от чего он не проигрывает, а только выигрывает в  читаемости.
Результат приведён на (lst. \ref{lst:gemmlocal}).

\begin{lstlisting}[caption={Перемножение матриц в локальной памяти},label={lst:gemmlocal}]
Sum = 0; N = AY / LSZ;
for (int Tile = 0; Tile < N; Tile++) {
  const int TX = LSZ * Tile + LX;
  const int TY = LSZ * Tile + LY;
  AL[TX][TY] = A[GX * AY + TX];
  BL[TX][TY] = B[TX * BY + GY];
  It.barrier(sycl_local_fence);
  for (int K = 0; K < LSZ; K++)
    Sum += AL[LX][K] * BL[K][LY];
  It.barrier(sycl_local_fence);
}
\end{lstlisting}

Замеры результатов работы такого кода приведены на (рис. \ref{fig:sgemm_lsz}).
Видно, что наконец-то применение локлаьной памяти дало ожидаемый эффект и MKL baseline был решительно превзойдён.
При этом использование максимально большого размера тайла улучшает производительность ещё сильнее.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{pictures/sgemm_lsz.pdf}
\caption{Замеры, показывающие положительный эффект локальной памяти на TGLLP}
\label{fig:sgemm_lsz}
\end{figure}

Можно ли ещё улучшить этот результат?
Да, если обратить внимание на один тонкий момент, а именно на то, что пока что здесь не обсуждался статус в памяти захваченных в кернел переменных.
Например на (lst. \ref{lst:gemmpayload}) показано, что переменная LSZ известна в момент запуска кернела и всю работу кернела не изменяется.
Но чем она является для кернела?

\begin{lstlisting}[caption={Захваченные переменные в payload},label={lst:gemmpayload}]
const int LSZ = LSZ_from_program_arguments;
.....
T *A = malloc_shared<T>(AX * AY, Queue);
std::copy(AH.begin(), AH.end(), A);
.....
Queue.submit([&](sycl::handler &Cgh) { 
  auto Kernmul = [=](sycl::id<2> Id) {    
    // here LSZ is used
\end{lstlisting}

Увы, возможности компилятора использовать такие переменные как есть сильно ограничены.
Фактически захваченная переменная является частью так называемого пейлоада (payload) -- списка неявных аргументов кернела.
То есть компилятор мог бы знать о ней всё, включая её значение, но он ничего не знает.
Идея как можно помочь оптимизатору, используя специализационные константы (specialization constants) показана на (lst. \ref{lst:gemmspec}).

\begin{lstlisting}[caption={Специализационные константы},label={lst:gemmspec}]
int LSZ = LSZ_from_program_arguments;
constexpr sycl::specialization_id<int> LSZC;
.....
DeviceQueue.submit([&](sycl::handler &Cgh) {
  Cgh.template set_specialization_constant<LSZC>(LSZ);
  auto Kernmul = [=](sycl::id<2> Id, sycl::kernel_handler Kh) {
    int Row = Id.get(0), Col = Id.get(1);    
    const int LSZK = Kh.get_specialization_constant<LSZC>();
    // here LSZK is used
\end{lstlisting}

Естественной критикой такого подхода является то, что теперь AOT компиляция заблокирована и программист не управляет моментом перекомпиляции кернела во время исполнения.
Это решается посредством использование kernel bundles.

% TODO: тут про kernel bundles

Итоговые замеры приведены на (рис. \ref{fig:sgemm_lsz_spec}) и являются потрясающими. Фактически MKL baseline превзойдён в четыре раза по производительности.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{pictures/sgemm_lsz_spec.pdf}
\caption{Замеры, показывающие комбо из локальной памяти и специализационных констант}
\label{fig:sgemm_lsz_spec}
\end{figure}

\textbf{Упражнение:} улучшат ли специализационные константы кернел без локальной памяти? Выскажите гипотезу, проведите замеры, посмотрите ассемблер, объясните результат.

\subsection{Битоническая сортировка}\label{subsec:bitonic}
% \subsection{Свёртки}\label{subsec:convolution}
% \subsection{Гистограмма}\label{subsec:hist}
% \subsection{Редукции}\label{subsec:reduction}

% \section{Атомарность в SYCL и в C++}\label{sec:Atomicity}

\end{document}
